[
  {
    "第5轮是否存在共情？"
    "第5轮回复共情程度是？"
    "第5轮共情对象是？" 
    "第5轮共情是否匹配"
    "第5轮共情不匹配原因"
    "第5轮共情程度是否准确？"
    "第5轮共情程度不准确原因"
    "第5轮共情对象是否准确？"
    "第5轮共情对象错误原因"
    "第10轮是否存在共情？"
    "第10轮共情程度是？"
    "第10轮共情程度是否准确？"
    "第10轮共情程度不准确原因"
    "第10轮共情对象是？"
    "第10轮教练共情对象是否准确？"
    "第10轮共情对象错误原因"
    "第10轮共情是否匹配"
    "第10轮共情不匹配原因"
    "第5轮积极关注是否使用"
    "第5轮积极关注的对象是？"
    "第5轮积极关注对象是否准确？"
    "第5轮积极关注对象错误原因"
    "第10轮积极关注是否使用"
    "第10轮中积极关注的对象是？"
    "第10轮积极关注对象是否准确？"
    "第10轮积极关注对象错误原因"
  }
]

我现在已经利用main py，把AI evalualor的评估结果写入了飞书多维表格了（its what I have done in main.py, you dont need to worry）
现在，我需要统计AI evaluator和人类专家评估结果的match度，统计结果不用写回去，在日志里记录就好。

统计的机制是这样的：
首先锁定who is human expert and who is AI evaluator. 
请在代码中开放一个参数，这个参数可以让我填入human expert和AI evaluator的名称。

match率的统计基本单位是1个对话，因为AI和人类在同一个对话中才share语境。语境之间是独立的。
也即是，你的match率统计，要先确认“对话编号”这个column name是一致的，才进行比较。
比较的条目如下：  {
    "第5轮是否存在共情？"
    "第5轮回复共情程度是？"
    "第5轮共情对象是？" 
    "第5轮共情是否匹配"
    "第5轮共情不匹配原因"
    "第5轮共情程度是否准确？"
    "第5轮共情程度不准确原因"
    "第5轮共情对象是否准确？"
    "第5轮共情对象错误原因"
    "第10轮是否存在共情？"
    "第10轮共情程度是？"
    "第10轮共情程度是否准确？"
    "第10轮共情程度不准确原因"
    "第10轮共情对象是？"
    "第10轮教练共情对象是否准确？"
    "第10轮共情对象错误原因"
    "第10轮共情是否匹配"
    "第10轮共情不匹配原因"
    "第5轮积极关注是否使用"
    "第5轮积极关注的对象是？"
    "第5轮积极关注对象是否准确？"
    "第5轮积极关注对象错误原因"
    "第10轮积极关注是否使用"
    "第10轮中积极关注的对象是？"
    "第10轮积极关注对象是否准确？"
    "第10轮积极关注对象错误原因"
  }


然后，请在代码中明确一个函数用来做alignment check

def alignment_check(id，item, human_expert_eval, ai_evaluator_eval):
'''
id 就是对话编号信息，int。
这里的item指的就是上面的条目（column names），会是str。可以换一个你觉得更专业的命名。你首先需要提取在某个id的某个item中，human_expert_eval和ai_evaluator_eval的值。
直接进行比较，如果一致，返回True，否则返回False。如果都是空值，也是返回True。如果一个有值一个是空，返回false。
'''

return bool

当你完成了所有id的所有条目的alignment check，返回统计率给我，百分比，两位小数保存。
以下是一个准确率统计的举例（具体数据不是真的），比如：
评估方法：平均准确率计算
我们采用平均准确率来评估AI的表现。具体计算方式如下：
评估对象：23份对话的共情维度评估
评估粒度：每份对话包含10个关于"共情"的检查点（checkpoint）
评估标准：比较AI评估结果与专家标准答案的一致性
计算公式：准确率 = 一致判断数量 / 总判断数量 = num(truth) / 230
其中：
总判断数量 = 23份对话 × 10个检查点 = 230个判断点
一致判断数量 = AI与专家意见相符的判断点数量（比如，第5轮共情是否匹配，如果一致，则计数+1）

====
同时，我也需要你返回一些mid-term分数，比如在某个id的准确率，在某个item的准确率。
我还需要你返回所有human和ai不一致的地方，in this format：(id,item,human_eval,ai_eval)

那么，飞书表格的数据怎么读呢（注意只是读，不需写回去），请参看：src/utils/feishu_client.py